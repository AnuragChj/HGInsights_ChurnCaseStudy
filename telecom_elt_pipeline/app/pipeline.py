# -*- coding: utf-8 -*-
"""pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xie_sI0X1qkbxY9F7Znrd_SKSoctw3e_
"""

import hashlib
import logging
from datetime import datetime

import pandas as pd
from sqlalchemy import text

from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine

import os
from dotenv import load_dotenv

load_dotenv()

DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = int(os.getenv("DB_PORT", "5432"))
DB_NAME = os.getenv("DB_NAME", "elt_db")
DB_USER = os.getenv("DB_USER", "elt_user")
DB_PASSWORD = os.getenv("DB_PASSWORD", "elt_password")

CSV_PATH = os.getenv("CSV_PATH", "/data/raw/telecom_churn.csv")

SCHEDULE_MINUTES = int(os.getenv("SCHEDULE_MINUTES", "60"))

SQLALCHEMY_ECHO = os.getenv("SQLALCHEMY_ECHO", "false").lower() == "true"

def get_engine() -> Engine:
    url = (
        f"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}"
        f"@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    )
    return create_engine(url, future=True)

def execute_sql(sql: str, **params):
    engine = get_engine()
    with engine.begin() as conn:
        conn.execute(text(sql), params)

logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
)

def hash_value(val) -> str | None:
    #One-way hash for anonymising PII (e.g., CustomerID).
    if pd.isna(val):
        return None
    return hashlib.sha256(str(val).encode("utf-8")).hexdigest()

def extract_csv() -> pd.DataFrame:
  #Read the data source to a CSV frame
    logger.info(f"Reading CSV from {CSV_PATH}")
    df = pd.read_csv(CSV_PATH)
    logger.info(f"Loaded {len(df)} rows from CSV")
    return df

def load_to_staging(df: pd.DataFrame):
  #Load raw dataframe into staging.telecom_churn_raw
    engine = get_engine()
    logger.info("Loading raw data into staging.telecom_customer_churn_raw")

    df.to_sql(
        "telecom_customer_churn_raw",
        schema="staging",
        con=engine,
        if_exists="replace",
        index=False,
    )
    logger.info("Loaded raw data into staging")

def transform(df: pd.DataFrame) -> pd.DataFrame:

    # Handling missing values
    #anonymise PII (CustomerID)
    #standardise column names/types


    logger.info("Starting transformation step")
    df = df.copy()


    # Whitespace removal
    df.columns = [c.strip() for c in df.columns]

    # Numeric columns
    numeric_cols = [ "MonthlyCharges", "TotalCharges"]
    categorical_cols = [c for c in df.columns if c not in numeric_cols]

    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
            df[col] = df[col].fillna(0)

    for col in categorical_cols:
        df[col] = df[col].fillna("Unknown")

    # PII anonymisation - hash CustomerID
    if "CustomerID" in df.columns:
        df["customer_hash"] = df["CustomerID"].apply(hash_value)
        df = df.drop(columns=["CustomerID"])

    # Match column names
    df = df.rename(
        columns={
    "CustomerID":"customer_id",
    "Age":"age",
    "Gender":"gender",
    "Tenure":"tenure",
    "MonthlyCharges":"monthly_charges",
    "ContractType":"contract_type",
    "InternetService":"internet_service",
    "TotalCharges":"total_charges",
    "TechSupport":"tech_support",
    "Churn":"churn"
        }
     )


    df["loaded_at"] = datetime.utcnow()

    logger.info("Transformation complete")
    return df

def load_to_analytics(df: pd.DataFrame):
    engine = get_engine()

    logger.info("Loading transformed data into analytics.telecom_customer_churn_clean")

    # Truncate and reload
    with engine.begin() as conn:
        conn.execute(text("TRUNCATE TABLE analytics.telecom_customer_churn_clean;"))
    df.to_sql(
        "telecom_customer_churn_clean",
        schema="analytics",
        con=engine,
        if_exists="append",
        index=False,
    )

    logger.info("Analytics table updated")

def run_pipeline():
    logger.info("--Pipeline started--")
    df_raw = extract_csv()
    load_to_staging(df_raw)

    df_transformed = transform(df_raw)
    load_to_analytics(df_transformed)

    logger.info("--Pipeline finished--")